{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPe3akdvFwOYucxyBmM20US"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jdXAAmlWDtOE"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","model_name_or_path = 'ckiplab/CKIP-Llama-2-7b-chat'\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# int8\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, load_in_8bit=True)\n","# fp16\n","# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16)\n","# fp32\n","# model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","model.to(device)\n","\n","sampling_strategy = {'max_new_tokens': 50,\n","                     'top_p': 0.95}\n","\n","prompt_template = \"Human: \\n{}\\n\\nAssistant: \\n\"\n","prompt = prompt_template.format('台灣最高的山？')\n","inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n","\n","output = model.generate(input_ids=inputs.input_ids, **sampling_strategy)\n","\n","print( tokenizer.batch_decode(output) )"]}]}