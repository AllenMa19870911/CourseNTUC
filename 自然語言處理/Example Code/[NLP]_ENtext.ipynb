{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMji+eNzrbhfkxfoRJgnqDK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#正規表達式\n","import re\n","text = 'Hello world. ~7- This is an apple.'\n","text2 = 'Hello world. ~-z This is  an apple.'\n","print(text.find(r'[6-9]-z'))\n","m = re.search(r'[abc]', text)\n","print(\"---------\")\n","print(m.group(0))\n","m = re.search(r'[6-9]-', text)\n","print(\"---------\")\n","print(m.group(0))\n","m = re.search(r'~[6-9]', text)\n","print(\"---------\")\n","print(m.group(0))\n","m = re.search(r'[6-9-z]', text2)\n","print(\"---------\")\n","print(m.group(0))\n","m = re.search(r'[1-a]', text)\n","print(\"---------\")\n","print(m.group(0))\n","m = re.search(r'[^A-Z]', text)\n","print(\"---------\")\n","print(m.group(0))\n","m = re.search(r'.', text)\n","print(\"---------\")\n","print(m.group(0))\n","m = re.search(r'[Hep|Hel]', text)\n","print(\"---------\")\n","print(m.group(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"py-4iVew2d58","executionInfo":{"status":"ok","timestamp":1678254527664,"user_tz":-480,"elapsed":319,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"a867f432-757b-48a5-a545-a7505fa2fb04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-1\n","---------\n","a\n","---------\n","7-\n","---------\n","~7\n","---------\n","-\n","---------\n","H\n","---------\n","e\n","---------\n","H\n","---------\n","H\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SlqZrgKHw_M","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"error","timestamp":1678254755814,"user_tz":-480,"elapsed":311,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"f2cec226-f67d-44a0-807e-13ca5df32165"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello world. This is an apple.\n","world. This is an apple.\n","H\n","world. This is an apple.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-dc033aff40bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Hello world. Thisss is an apple.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'is{4,5}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"]}],"source":["import re\n","text = 'Hello world. This is an apple.'\n","m = re.match(r'H.+', text)\n","print(m.group(0))\n","m = re.search(r'w.+', text)\n","print(m.group(0))\n","m = re.search(r'\\w?', text)\n","print(m.group(0))\n","m = re.search(r'w.*', text)\n","print(m.group(0))\n","text = 'Hello world. Thisss is an apple.'\n","m = re.search(r'is{4,5}', text)\n","print(m.group(0))\n","\n"]},{"cell_type":"code","source":["import re\n","text = 'Hello world. This is an apple.'\n","substrs = re.findall(r'\\w+', text)\n","print(substrs)\n","new_text = re.sub('o','-', text)\n","print(new_text)\n","new_text, sub_count = re.subn ('o', '-', text,1)\n","print(new_text, sub_count)\n","new_text, sub_count = re.subn ('o', '-', text)\n","print(new_text, sub_count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUdWELsvIhs0","executionInfo":{"status":"ok","timestamp":1678242592177,"user_tz":-480,"elapsed":432,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"7da3716a-ba74-4a80-de70-e8f213f268d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'world', 'This', 'is', 'an', 'apple']\n","Hell- w-rld. This is an apple.\n","Hell- world. This is an apple. 1\n","Hell- w-rld. This is an apple. 2\n"]}]},{"cell_type":"code","source":["#re簡易資料清理和斷詞\n","import re\n","import os\n","with open(os.getcwd()+\"/sample_data/entext.txt\") as f:\n","  s = f.read()\n","  slist= s.split(\"\\n\")\n","  for text_data in slist:\n","    text_data = re.sub('[^0-9a-zA-Z]', ' ', text_data)\n","    text_data = text_data.lower()\n","    text_data = text_data.split()\n","    print(text_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fRKxDfKTCxZ","executionInfo":{"status":"ok","timestamp":1678256728987,"user_tz":-480,"elapsed":270,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"2bcfa39f-50a5-4e30-b707-34e0e70d89f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['research', 'and', 'ads', 'tracks']\n","['this', 'is', 'a', 'joint', 'call', 'for', 'papers', 'for', 'the', 'research', 'and', 'the', 'applied', 'data', 'science', 'tracks', 'at', 'ecmlpkdd', '2023']\n","[]\n","['in', 'the', 'research', 'track', 'we', 'invite', 'submissions', 'of', 'research', 'papers', 'from', 'all', 'areas', 'of', 'knowledge', 'discovery', 'data', 'mining', 'and', 'machine', 'learning', 'we', 're', 'looking', 'for', 'high', 'quality', 'papers', 'in', 'terms', 'of', 'novelty', 'technical', 'quality', 'potential', 'impact', 'and', 'clarity', 'of', 'presentation', 'papers', 'should', 'demonstrate', 'that', 'they', 'make', 'a', 'significant', 'contribution', 'to', 'the', 'field', 'e', 'g', 'improve', 'the', 'state', 'of', 'the', 'art', 'or', 'provide', 'a', 'new', 'theoretical', 'insight']\n","[]\n","['in', 'the', 'applied', 'data', 'science', 'track', 'we', 'invite', 'submissions', 'that', 'present', 'compelling', 'applications', 'of', 'knowledge', 'discovery', 'data', 'mining', 'and', 'machine', 'learning', 'to', 'solve', 'challenging', 'and', 'important', 'real', 'world', 'problems', 'thereby', 'bridging', 'the', 'gap', 'between', 'practice', 'and', 'theory', 'papers', 'should', 'clearly', 'explain', 'the', 'real', 'world', 'challenge', 'addressed', 'including', 'any', 'peculiarities', 'of', 'the', 'data', 'such', 'as', 'the', 'size', 'of', 'the', 'dataset', 'or', 'noise', 'levels', 'the', 'methodology', 'used', 'and', 'the', 'conclusions', 'and', 'implications', 'that', 'are', 'drawn', 'for', 'the', 'use', 'case', 'if', 'the', 'paper', 'presents', 'a', 'deployed', 'solution', 'explicitly', 'mention', 'it', 'in', 'the', 'paper', 'and', 'provide', 'any', 'relevant', 'details']\n","[]\n","['key', 'dates', 'and', 'deadlines']\n","['abstract', 'submission', 'deadline', 'march', '26', '2023']\n","['paper', 'submission', 'deadline', 'april', '2', '2023']\n","['author', 'notification', 'june', '5', '2023']\n","['camera', 'ready', 'submission', 'june', '23', '2023']\n","['all', 'deadlines', 'expire', 'at', '23', '59', 'aoe', 'utc', '12']\n","['conference', '18', '22', 'september', '2023']\n","['paper', 'format']\n","['papers', 'must', 'be', 'written', 'in', 'english', 'and', 'formatted', 'according', 'to', 'the', 'springer', 'lncs', 'guidelines', 'author', 'instructions', 'style', 'files', 'and', 'the', 'copyright', 'form', 'can', 'be', 'downloaded', 'here']\n","[]\n","['the', 'maximum', 'length', 'of', 'papers', 'is', '14', 'pages', 'for', 'the', 'technical', 'content', 'excluding', 'references', 'in', 'this', 'format', 'the', 'space', 'for', 'references', 'is', 'not', 'limited', 'in', 'addition', 'there', 's', 'one', 'page', 'for', 'the', 'discussion', 'of', 'ethical', 'issues', 'ethical', 'statement', 'see', 'below', 'papers', 'that', 'exceed', 'this', 'limit', 'will', 'be', 'desk', 'rejected', 'egregious', 'changes', 'to', 'the', 'format', 'to', 'cheat', 'the', 'page', 'limit', 'may', 'also', 'lead', 'to', 'desk', 'rejection']\n","[]\n","['up', 'to', '10', 'mb', 'of', 'additional', 'materials', 'e', 'g', 'proofs', 'audio', 'images', 'video', 'data', 'or', 'source', 'code', 'can', 'be', 'uploaded', 'with', 'your', 'submission', 'the', 'reviewers', 'and', 'the', 'program', 'committee', 'reserve', 'the', 'right', 'to', 'judge', 'the', 'paper', 'solely', 'on', 'the', 'basis', 'of', 'the', 'main', 'paper', 'looking', 'at', 'any', 'additional', 'material', 'is', 'at', 'the', 'discretion', 'of', 'the', 'reviewers', 'and', 'is', 'not', 'required']\n","[]\n","['all', 'papers', 'need', 'to', 'be', 'best', 'effort', 'anonymized', 'we', 'strongly', 'encourage', 'making', 'code', 'and', 'data', 'available', 'anonymously', 'e', 'g', 'in', 'an', 'anonymous', 'github', 'repository', 'via', 'anonymous', 'github', 'or', 'in', 'a', 'dropbox', 'folder', 'the', 'authors', 'may', 'have', 'a', 'non', 'anonymous', 'pre', 'print', 'published', 'online', 'but', 'it', 'should', 'not', 'be', 'cited', 'in', 'the', 'submitted', 'paper', 'to', 'preserve', 'anonymity', 'reviewers', 'will', 'be', 'asked', 'not', 'to', 'search', 'for', 'them']\n","[]\n","['submission', 'process']\n","['electronic', 'submissions', 'will', 'be', 'handled', 'via', 'cmt', 'available', 'at', 'the', 'following', 'address', 'https', 'cmt3', 'research', 'microsoft', 'com', 'ecmlpkdd2023', 'before', 'submitting', 'please', 'consider', 'carefully', 'what', 'the', 'appropriate', 'track', 'is', 'see', 'the', 'top', 'of', 'the', 'page', 'the', 'track', 'of', 'choice', 'can', 'be', 'indicated', 'in', 'the', 'submission', 'form', 'submissions', 'will', 'be', 'assessed', 'in', 'the', 'track', 'where', 'they', 'were', 'submitted', 'and', 'cannot', 'be', 'transferred', 'across', 'tracks']\n","[]\n","['double', 'blind', 'reviewing', 'process']\n","['submissions', 'will', 'be', 'evaluated', 'by', 'at', 'least', 'three', 'reviewers', 'on', 'the', 'basis', 'of', 'relevance', 'technical', 'quality', 'potential', 'impact', 'and', 'clarity', 'ecmlpkdd', 'has', 'a', 'long', 'standing', 'reputation', 'for', 'being', 'a', 'truly', 'diverse', 'conference', 'where', 'many', 'topics', 'in', 'machine', 'learning', 'and', 'data', 'mining', 'are', 'represented', 'thus', 'the', 'selection', 'process', 'will', 'also', 'take', 'this', 'factor', 'into', 'consideration']\n","[]\n","['the', 'reviewing', 'process', 'is', 'double', 'blind', 'reviewers', 'and', 'area', 'chairs', 'are', 'not', 'aware', 'of', 'the', 'identities', 'of', 'the', 'authors', 'reviewers', 'can', 'see', 'each', 'other', 's', 'names', 'papers', 'must', 'not', 'include', 'identifying', 'information', 'of', 'the', 'authors', 'names', 'affiliations', 'etc', 'self', 'references', 'or', 'links', 'e', 'g', 'github', 'youtube', 'that', 'reveal', 'the', 'authors', 'identities', 'e', 'g', 'references', 'to', 'own', 'work', 'should', 'be', 'given', 'neutrally', 'like', 'other', 'references', 'not', 'mentioning', 'our', 'previous', 'work', 'or', 'similar', 'however', 'we', 'recognize', 'there', 'are', 'limits', 'to', 'what', 'is', 'feasible', 'with', 'respect', 'to', 'anonymization', 'for', 'example', 'if', 'you', 'use', 'data', 'from', 'your', 'own', 'organization', 'and', 'it', 'is', 'relevant', 'to', 'the', 'paper', 'to', 'name', 'this', 'organization', 'you', 'may', 'do', 'so']\n"]}]},{"cell_type":"code","source":["#簡易nltk斷詞\n","import nltk\n","import os\n","#nltk.download('punkt')\n","#paragragh=\"As knowledge graphs have attracted enormous attention from researchers research, much effort has been invested in recommendation systems to mine user preferences effectively. In particular, knowledge graphs, which convey useful side information about users and items, can provide more accurate and explainable recommendations. When it comes to interactions between entities, however, the majority of existing work fails to incorporate high-order relations that ensure recommendation accuracy. This paper proposes attention-enhanced joint knowledge and user preference propagation (AKUPP), which integrates two types of knowledge propagation. The first is propagating user preferences based on the users' history of interacting items through ripple sets. The second propagation employs an attention mechanism to emphasize the important semantics of relations, and with multiple layers, high-order relations are explored. Therefore, we successfully incorporate both side information and high-order relations in the knowledge graph. We show, via extensive experimentation on real-world datasets, that our approach outperforms numerous state-of-the-art baselines in terms of performance and accuracy.\"\n","with open(os.getcwd()+\"/sample_data/entext.txt\") as f:\n","  paragragh = f.read()\n","  sentlist = nltk.sent_tokenize(paragragh)\n","  for sent in sentlist:\n","    print(sent)\n","    words = nltk.word_tokenize(sent)\n","    print(words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":387},"id":"H_vN0ytVOnWI","executionInfo":{"status":"error","timestamp":1696563780259,"user_tz":-480,"elapsed":1834,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"34129802-7051-4b95-fd40-58115f435c52"},"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-72012ed45008>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#nltk.download('punkt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#paragragh=\"As knowledge graphs have attracted enormous attention from researchers research, much effort has been invested in recommendation systems to mine user preferences effectively. In particular, knowledge graphs, which convey useful side information about users and items, can provide more accurate and explainable recommendations. When it comes to interactions between entities, however, the majority of existing work fails to incorporate high-order relations that ensure recommendation accuracy. This paper proposes attention-enhanced joint knowledge and user preference propagation (AKUPP), which integrates two types of knowledge propagation. The first is propagating user preferences based on the users' history of interacting items through ripple sets. The second propagation employs an attention mechanism to emphasize the important semantics of relations, and with multiple layers, high-order relations are explored. Therefore, we successfully incorporate both side information and high-order relations in the knowledge graph. We show, via extensive experimentation on real-world datasets, that our approach outperforms numerous state-of-the-art baselines in terms of performance and accuracy.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/sample_data/entext.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mparagragh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0msentlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagragh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/entext.txt'"]}]},{"cell_type":"code","source":["#nltk客製化斷詞\n","from nltk.tokenize import RegexpTokenizer\n","string = \"I'm very happy because I earn so much money today.\"\n","tokenizer = RegexpTokenizer(r'\\w+', gaps = False)\n","clean_sent = tokenizer.tokenize(string)\n","print(clean_sent)\n","tokenizer2 = RegexpTokenizer(r'\\w+|\\'\\w+', gaps = False)\n","clean_sent2 = tokenizer2.tokenize(string)\n","print(clean_sent2)\n","tokenizer3 = RegexpTokenizer(r'\\w+|\\'\\w+', gaps = True)\n","clean_sent3 = tokenizer3.tokenize(string)\n","print(clean_sent3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDCFoTVgsGIC","executionInfo":{"status":"ok","timestamp":1678258118926,"user_tz":-480,"elapsed":260,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"49f8d4be-fd9d-48c0-8497-6d14a47d699f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'm', 'very', 'happy', 'because', 'I', 'earn', 'so', 'much', 'money', 'today']\n","['I', \"'m\", 'very', 'happy', 'because', 'I', 'earn', 'so', 'much', 'money', 'today']\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '.']\n"]}]},{"cell_type":"code","source":["#nltk stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5pfNtGEyNvq","executionInfo":{"status":"ok","timestamp":1677902544367,"user_tz":-480,"elapsed":271,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"0dbe2681-fe0e-4668-c791-1d86d7da4a7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["#nltk stopwords\n","stopword = stopwords.words('english')\n","print(stopword[1:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57cgaILiq3bb","executionInfo":{"status":"ok","timestamp":1677903341725,"user_tz":-480,"elapsed":277,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"2b6a1d8f-63c9-434a-d3df-05c20c00d3c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once']\n"]}]},{"cell_type":"code","source":["#nltk 斷詞後移除stopwords\n","from nltk.tokenize import RegexpTokenizer\n","paragraph=\"We've showed, via extensive experimentation on real-world datasets, that our approach outperforms numerous state-of-the-art baselines in terms of performance and accuracy.\"\n","tokenizer2 = RegexpTokenizer(r'\\w+|\\'\\w+', gaps = False)\n","clean_sent2 = tokenizer2.tokenize(paragraph)\n","for i in clean_sent2:\n","  if i in stopword:\n","    print(i)\n","    clean_sent2.remove(i)\n","print(clean_sent2)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p53mzv393v0B","executionInfo":{"status":"ok","timestamp":1677904427004,"user_tz":-480,"elapsed":262,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"61aadb8c-af6c-4a82-cb6a-a93c3ba4cecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["on\n","that\n","of\n","in\n","of\n","and\n","['We', \"'ve\", 'showed', 'via', 'extensive', 'experimentation', 'real', 'world', 'datasets', 'our', 'approach', 'outperforms', 'numerous', 'state', 'the', 'art', 'baselines', 'terms', 'performance', 'accuracy']\n"]}]},{"cell_type":"code","source":["#nltk移除stopwords\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import RegexpTokenizer\n","paragraph=\"We've showed, via extensive experimentation on real-world datasets, that our approach outperforms numerous state-of-the-art baselines In terms of performance and accuracy.\"\n","tokenizer2 = RegexpTokenizer(r'\\w+|\\'\\w+', gaps = False)\n","raw_result = nltk.word_tokenize(paragraph)\n","punct_result = tokenizer2.tokenize(paragraph)\n","result = []\n","for i in punct_result:\n","    if i not in stopword:\n","        token = i\n","        result.append(token)\n","print(result[1:50])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JlKjYbY905Ey","executionInfo":{"status":"ok","timestamp":1677903847221,"user_tz":-480,"elapsed":251,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"45e1f1fb-2b61-4019-ec7f-0be1c2b5b8c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[\"'ve\", 'showed', 'via', 'extensive', 'experimentation', 'real', 'world', 'datasets', 'approach', 'outperforms', 'numerous', 'state', 'art', 'baselines', 'In', 'terms', 'performance', 'accuracy']\n"]}]},{"cell_type":"code","source":["#nltk stemming\n","import nltk\n","from nltk.stem.porter import PorterStemmer\n","ps=PorterStemmer()\n","list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n","         'driving', 'died', 'tried', 'feet']\n","for words in list1:\n","    print(words + \" ---> \" + ps.stem(words))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZrQSUlPG4s9","executionInfo":{"status":"ok","timestamp":1677916029797,"user_tz":-480,"elapsed":242,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"9fefdabe-84c9-4809-b855-24faae3cce44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["kites ---> kite\n","babies ---> babi\n","dogs ---> dog\n","flying ---> fli\n","smiling ---> smile\n","driving ---> drive\n","died ---> die\n","tried ---> tri\n","feet ---> feet\n"]}]},{"cell_type":"code","source":["#nltk lemmatization\n","import nltk\n","nltk.download('omw-1.4')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","\n","# Create WordNetLemmatizer object\n","wnl = WordNetLemmatizer()\n","\n","# single word lemmatization examples\n","list1 = ['kites', 'babies', 'dogs', 'flies', 'smiling',\n","         'driving', 'died', 'tried', 'feet']\n","for words in list1:\n","    print(words + \" ---> \" + wnl.lemmatize(words))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rp6UKMC8FU61","executionInfo":{"status":"ok","timestamp":1677916256521,"user_tz":-480,"elapsed":3,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"0d847089-5327-4965-e430-62ae885a17f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["kites ---> kite\n","babies ---> baby\n","dogs ---> dog\n","flies ---> fly\n","smiling ---> smiling\n","driving ---> driving\n","died ---> died\n","tried ---> tried\n","feet ---> foot\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import wordnet\n","lemmatizer = WordNetLemmatizer()\n","list1 = ['kites', 'babies', 'dogs', 'flies', 'smiling',\n","         'driving', 'died', 'tried', 'feet']\n","print(\"######Lemma by V######\")\n","for words in list1:\n","  lemma = lemmatizer.lemmatize(words,'v')\n","  print(words + \" ---> \" + lemma)\n","print(\"######Lemma by N######\")\n","for words in list1:\n","  lemma = lemmatizer.lemmatize(words,'n')\n","  print(words + \" ---> \" + lemma)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEXTW4MjjhaJ","executionInfo":{"status":"ok","timestamp":1677917674146,"user_tz":-480,"elapsed":225,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"509fe267-fc0f-4682-b8d4-32cf23bbf8b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["######Lemma by V######\n","kites ---> kit\n","babies ---> baby\n","dogs ---> dog\n","flies ---> fly\n","smiling ---> smile\n","driving ---> drive\n","died ---> die\n","tried ---> try\n","feet ---> feet\n","######Lemma by N######\n","kites ---> kite\n","babies ---> baby\n","dogs ---> dog\n","flies ---> fly\n","smiling ---> smiling\n","driving ---> driving\n","died ---> died\n","tried ---> tried\n","feet ---> foot\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["#nltk詞性標註\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('averaged_perceptron_tagger')\n","paragraph=\"We've showed, via extensive experimentation on real-world datasets, that our approach outperforms numerous state-of-the-art baselines in terms of performance and accuracy.\"\n","pos_tagged = nltk.pos_tag(nltk.word_tokenize(paragraph))\n","print(pos_tagged)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oSNgOLFk5PD","executionInfo":{"status":"ok","timestamp":1677926708834,"user_tz":-480,"elapsed":283,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"8bee78d0-e7cb-4357-9940-3eb5ca91b19a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('We', 'PRP'), (\"'ve\", 'VBP'), ('showed', 'VBN'), (',', ','), ('via', 'IN'), ('extensive', 'JJ'), ('experimentation', 'NN'), ('on', 'IN'), ('real-world', 'JJ'), ('datasets', 'NNS'), (',', ','), ('that', 'IN'), ('our', 'PRP$'), ('approach', 'NN'), ('outperforms', 'VBZ'), ('numerous', 'JJ'), ('state-of-the-art', 'JJ'), ('baselines', 'NNS'), ('in', 'IN'), ('terms', 'NNS'), ('of', 'IN'), ('performance', 'NN'), ('and', 'CC'), ('accuracy', 'NN'), ('.', '.')]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["#nltk詞性標註結合lemmatization\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('averaged_perceptron_tagger')\n","paragraph=\"We've showed, via extensive experimentation on real-world datasets, that our approach outperforms numerous state-of-the-art baselines in terms of performance and accuracy.\"\n","pos_tagged = nltk.pos_tag(nltk.word_tokenize(paragraph))\n","#print(pos_tagged)\n","wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n","print(wordnet_tagged)\n","def pos_tagger(nltk_tag):\n","    if nltk_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif nltk_tag.startswith('N'):\n","        return wordnet.NOUN\n","    else:\n","        return wordnet.NOUN\n","for words in wordnet_tagged:\n","  lemma = lemmatizer.lemmatize(words[0], words[1])\n","  print(words[0] + \" ---> \" + lemma)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfNgUQ_SsYpR","executionInfo":{"status":"ok","timestamp":1677919413041,"user_tz":-480,"elapsed":2,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"ac5745ae-d759-4995-e9e9-e3d82035bf2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('We', 'n'), (\"'ve\", 'v'), ('showed', 'v'), (',', 'n'), ('via', 'n'), ('extensive', 'n'), ('experimentation', 'n'), ('on', 'n'), ('real-world', 'n'), ('datasets', 'n'), (',', 'n'), ('that', 'n'), ('our', 'n'), ('approach', 'n'), ('outperforms', 'v'), ('numerous', 'n'), ('state-of-the-art', 'n'), ('baselines', 'n'), ('in', 'n'), ('terms', 'n'), ('of', 'n'), ('performance', 'n'), ('and', 'n'), ('accuracy', 'n'), ('.', 'n')]\n","We ---> We\n","'ve ---> 've\n","showed ---> show\n",", ---> ,\n","via ---> via\n","extensive ---> extensive\n","experimentation ---> experimentation\n","on ---> on\n","real-world ---> real-world\n","datasets ---> datasets\n",", ---> ,\n","that ---> that\n","our ---> our\n","approach ---> approach\n","outperforms ---> outperform\n","numerous ---> numerous\n","state-of-the-art ---> state-of-the-art\n","baselines ---> baseline\n","in ---> in\n","terms ---> term\n","of ---> of\n","performance ---> performance\n","and ---> and\n","accuracy ---> accuracy\n",". ---> .\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["#詞頻統計\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.probability import FreqDist\n","nltk.download('wordnet')\n","paragraph=\"We've showed, via extensive experimentation on real-world datasets, that our approach outperforms numerous state-of-the-art baselines in terms of performance and accuracy.\"\n","\n","def lemmatize(word):\n","    lemma = lemmatizer.lemmatize(word,'v')\n","    if lemma == word:\n","        lemma = lemmatizer.lemmatize(word,'n')\n","    return lemma\n","\n","tokenizer2 = RegexpTokenizer(r'\\w+|\\'\\w+', gaps = False)\n","punct_result = tokenizer2.tokenize(paragraph)\n","lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n","words = [lemmatize(token) for token in punct_result]\n","print(words)\n","fdist = FreqDist(words)\n","print(fdist.most_common())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_NtMocv3d1C","executionInfo":{"status":"ok","timestamp":1677921199826,"user_tz":-480,"elapsed":3,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"ef68db1e-bbca-4a33-a5df-5f75aa4bf087"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['We', \"'ve\", 'show', 'via', 'extensive', 'experimentation', 'on', 'real', 'world', 'datasets', 'that', 'our', 'approach', 'outperform', 'numerous', 'state', 'of', 'the', 'art', 'baseline', 'in', 'term', 'of', 'performance', 'and', 'accuracy']\n","[('of', 2), ('We', 1), (\"'ve\", 1), ('show', 1), ('via', 1), ('extensive', 1), ('experimentation', 1), ('on', 1), ('real', 1), ('world', 1), ('datasets', 1), ('that', 1), ('our', 1), ('approach', 1), ('outperform', 1), ('numerous', 1), ('state', 1), ('the', 1), ('art', 1), ('baseline', 1), ('in', 1), ('term', 1), ('performance', 1), ('and', 1), ('accuracy', 1)]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["#語料庫下載\n","import nltk\n","from nltk.corpus import gutenberg\n","nltk.download('gutenberg')\n","print(gutenberg.fileids())\n","#print(gutenberg.raw('austen-emma.txt'))\n","#print(gutenberg.words('austen-emma.txt'))\n","#print(gutenberg.sents('austen-emma.txt'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IWbZnfqmAtjJ","executionInfo":{"status":"ok","timestamp":1678259371160,"user_tz":-480,"elapsed":2,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"d55ab736-56f3-48a7-f343-c5dc079c9d60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]}]},{"cell_type":"code","source":["#多個文件計算詞頻\n","from nltk.probability import ConditionalFreqDist\n","from nltk.corpus import brown\n","genres = ['reviews', 'hobbies', 'romance', 'humor']\n","modals = ['can', 'could', 'may', 'might', 'must', 'will']\n","cfd = ConditionalFreqDist()\n","cfdist = ConditionalFreqDist((len(word), word) for word in brown.words(categories=genres))\n","print(cfdist[3]['the'])\n","for word in brown.words(categories=genres):\n","  condition = len(word)\n","  cfd[condition][word] += 1\n","print(cfd[4]['must'])\n","#cfd = nltk.ConditionalFreqDist((text,word)for word in brown.words())\n","#cfd = nltk.ConditionalFreqDist((genre, word)\n","#    for genre in brown.categories()\n","#    for word in brown.words(categories=genre))\n","\n","#cfd.tabulate(conditions=genres, samples=modals)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIxyGbqKFozv","executionInfo":{"status":"ok","timestamp":1677925661187,"user_tz":-480,"elapsed":3158,"user":{"displayName":"Allen Ma","userId":"06919684488186026564"}},"outputId":"dd526122-2a62-42f5-c5fd-997455cd7f63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['It', 'is', 'not', 'news', 'that', 'Nathan', ...]\n","10036\n","156\n"]}]}]}